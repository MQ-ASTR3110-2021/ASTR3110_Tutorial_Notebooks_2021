{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR3110 Tutorial 4: MCMC\n",
    "\n",
    "Tutorial 4 of the *'Data Science Techniques in Astrophysics'* course at Macquarie University.\n",
    "\n",
    "## Learning outcomes from this tutorial\n",
    "\n",
    " * Understand what a sampler achieves\n",
    " * Understand the terms in Bayes's Formula and how they relate to fitting models\n",
    " * Use a MCMC sampler to fit a polynomial model to a simple 1D spectrum\n",
    " * Create a triangle plot to show correlations between parameters\n",
    " * Extract best-fit and uncertainty estimates from an MCMC chain\n",
    " * Apply the Nested Sampling algorithm to the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood, Priors and Bayes's Theorem\n",
    "\n",
    "In Tutorial 3 we used $\\chi^2_{\\rm model}$ difference between the model and data as a measure of *goodness of fit* and found the best-fitting model by minimizing its value. However, we often have *prior* independent measurements of the parameters we want to fit. We want this prior information to weight the total $\\chi^2$ value so that it prefers parameter values closer to our previous measurements. Each prior parameter estimate contributes its own $\\chi^2$ value. For example, for a parameter $p = p_{\\rm prior} \\pm \\sigma_{\\rm prior}$, the $\\chi^2_{\\rm prior}$ value is given by:\n",
    "\n",
    "$$\\chi^2_{\\rm prior} = \\left(\\frac{p_{\\rm model} - p_{\\rm prior}}{\\sigma_{\\rm prior}}\\right)^2$$.\n",
    "\n",
    "The total $\\chi^2$ value is then given by the sum of all values: $\\chi^2 = \\chi^2_{\\rm model} + \\chi^2_{\\rm prior}$.\n",
    "\n",
    "The *likelihood* is a related way of assessing a model fit: *what is the probability, or likelhood $\\mathcal{L}$, of getting the observed data given particular values of parameters p?* For Gaussian distributed likelihoods the likelihood is related to $\\chi^2$ via\n",
    "\n",
    "$$\\mathcal{L} = exp(-\\chi^2\\big/2)~~~~or~~~~-2~{\\rm ln}(\\mathcal{L}) = \\chi^2,$$\n",
    "\n",
    "so that $\\chi^2$ is often referred to as the 'log-likelihood'. Because $\\mathcal{L}$ is in log-space, likelihoods are *multiplied* to get the total $\\mathcal{L}$. From a numerical perspective, the log-likelihood is also a smaller number and less likely to run into computational limits.\n",
    "\n",
    "### Bayes's Theorem\n",
    "\n",
    "For a model hypothesis $\\mathcal{M}$\n",
    "and data $\\boldsymbol{d}$, the posterior probability for a set of\n",
    "model parameters $\\boldsymbol{\\theta}$ is given by Bayes' Theorem\n",
    "\\begin{equation}\\label{eqn:bayes_formula}\n",
    "  {\\rm Pr}(\\boldsymbol{\\theta}|\\boldsymbol{d}, \\mathcal{M}) =\n",
    "  \\frac{{\\rm Pr}(\\boldsymbol{d}|\\boldsymbol{\\theta},\n",
    "    \\mathcal{M})\\,{\\rm Pr}(\\boldsymbol{\\theta}|\\mathcal{M})}\n",
    "       {{\\rm Pr}(\\boldsymbol{d}|\\mathcal{M})}.\n",
    "\\end{equation}\n",
    "The term ${\\rm Pr}(\\boldsymbol{d}|\\boldsymbol{\\theta}, \\mathcal{M})$\n",
    "is the probability of the data given the model parameters, also known\n",
    "as the likelihood $\\mathcal{L}$.  The term ${\\rm Pr}(\\boldsymbol{\\theta}|\\mathcal{M})$ is known as the\n",
    "prior; it encodes the probability of the\n",
    "current parameter values given the proposed model, i.e., any  *a\n",
    "  priori* information about the parameter values. The final term ${\\rm Pr}(\\boldsymbol{d}|\\mathcal{M})$, known as the\n",
    "evidence, is the probability of the data given the model and serves to\n",
    "normalise the posterior distribution so that the total probability is\n",
    "unity.\n",
    "\n",
    "I found the discussion by Adrian Price-Whelan of the Flatiron Institute to be very helpful for understanding the above: [see this link](https://adrian.pw/blog/fitting-a-line/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MCMC Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Drive and data access\n",
    "\n",
    "As with the tutorial last week, we will be operating on actual data, so please mount your Gooogle Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out if running locally\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into the correct directory (replace with the location of your copy)\n",
    "#cd \"gdrive/\"My Drive\"/ASTR3110_Tutorial_Notebooks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be processing the same polarised radio-wavelength spectrum as last week. If you haven't already downloaded it, you can get it from this link: [HotSpot.dat](https://github.com/MQ-ASTR3110-2020/ASTR3110_Tutorial_Notebooks/blob/master/DATA/HotSpot.dat). Save this file to your Google Drive under, for example, your 'DATA/' directory.\n",
    "\n",
    "As a reminder, the file contains 7 columns corresponding to:\n",
    "\n",
    "[frequency_GHz, I_mJy, Q_mJy, U_mJy, dI_mJy, dQ_mJy, dU_mJy]\n",
    "\n",
    "and we want to access the columns [0] frequency, [1] flux and [4] uncertainty in flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Array has shape (columns, rows):', (7, 198))\n"
     ]
    }
   ],
   "source": [
    "#Same as Tutorial 3\n",
    "import numpy as np\n",
    "\n",
    "# Read all columns into an Numpy array\n",
    "specArr  = np.loadtxt(\"DATA/HotSpot.dat\", dtype=\"float64\", unpack=True)\n",
    "\n",
    "# Convert Hz to GHz\n",
    "specArr[0] /= 1e9\n",
    "print(\"Array has shape (columns, rows):\", specArr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shortcut, some useful functions are in the file ```Imports/util_tute04.py```. These can be imported into the current notebook like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to evaluate a polynomial\n",
    "from Imports.util_tute04 import polyN\n",
    "\n",
    "# Import the function to plot the spectrum\n",
    "from Imports.util_tute04 import plot_spec_polyN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine *code* by using double question-marks to summon help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the code for the imported function (show 1 ?, which gives short description vs ??, which gives code.)\n",
    "#polyN?\n",
    "#polyN??\n",
    "#plot_spec_polyN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this is our convenience function from Tutorial 3 for plotting the polynomial data and (optionally) best-fitting model. Use it now to check the data has been read in correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Set the data arrays from the file\n",
    "xData = specArr[0]\n",
    "yData = specArr[1]\n",
    "dyData = specArr[4]\n",
    "\n",
    "# Plot the spectrum\n",
    "plot_spec_polyN(xData, yData, dyData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MCMC to fit a Polynomial\n",
    "\n",
    "Here we use the MCMC module ```emcee``` to fit our familiar spectrum using a polynomial model. Using a MCMC sampler has some distinct advantages when it comes to estimating the best-fitting parameters and uncertainty values.\n",
    "\n",
    "Like before we define a *goodness-of-fit* function. However, this time we formulate it as a log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a gradient-decent method, we are not trying to go directly to a minimum point. Instead we want our guesses to spread out over paramter space *near the best-fit value*. Because the next guess of the chain is weighted by the likelihood, the 'walkers' spend most time near the peak likelihood and less time away from it. The density of samples can be used as a proxy for the posterior likelihood distribution.\n",
    "\n",
    "For this excercise we will use a module called [emcee](https://emcee.readthedocs.io/en/stable/). This is a pure *Python* implementation of an MCMC sampler that makes many guesses in parallel. You can visualise each guess as a time-series of positions in parameter space that 'walk' all over parameter space - hence we call them 'walkers'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set our sampler parameters we can set it off walking to the answer. We have made no attempt at making a good initial guess, so our walkers will probably take a while to get close to the best fitting region. This is called the burn-in period and you need to visualise the history of each walker to see when they reach the right area of parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampler returns three things:\n",
    "\n",
    " * pos =   the last position of each sampler\n",
    " * prob =  the last likelihood for each sampler\n",
    " * state = the current state of the random number generator\n",
    " \n",
    "The sampler also stores the history of each walker internally - refered to as a chain. We need to visualise the chains for each parameter to see how long it takes *all parameters* to burn-in. Let's define a function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have made an nice version of this function in the file ```Imports/util_tute04.py```, so you can use that in future. This function plots the samples as points, coloured by likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your initial values, most chains have settled down to a flat (but finitely thick) trace. If they have not then you should run the burn-in for more steps.\n",
    "\n",
    "Now we want to run the sampler again, but initializing each walker with the converged values, that is, the values on the extreme right of the plot above. These are stored in the ```pos``` variable from the last run of the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Point:** \n",
    "\n",
    "Looking at a single plot above (e.g., the P1 parameter), we see that the samples are scattered around a single value of ~0.51. A histogram of sampled values is proportional to the *posterior likelihood distribution* of the P1 parameter.\n",
    "\n",
    "Start by accessing a *flattened* version of the chain: all 100 parallel walker chains merged into one 1D array. Then plot the histogram of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make estimates of the best-fit value and uncertainties using the method described in Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise how each parameter is correlated with every other parameter. Let's try checking how the polynomial coefficent P6 is correlated with P1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the P6 is anti-correlated with P1: solutions with higher values of P6 drive the P1 value down. \n",
    "\n",
    "Rather than making these plots by hand, there is a module called ```corner``` that is dedicated to plotting the results of MCMC samplers and similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should calculate the best-fitting results and plot the model over the data as a visual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
